# Ceph 安装心得

## 搭建环境与安装

* ### 搭建ceph节点

  首先一个ceph集群至少需要3个OSD才能实现冗余和高可用性，并且至少需要一个mon和一个msd。其中mon和msd可以部署在其中一个osd节点上，所以在本次实验中选择建立三个虚拟机主机作为三个节点，再建立一个作为ceph客户端。

  主机建立：

  在vmware中，将安装好的ubuntu18虚拟机进行克隆，将虚拟机的名称改为ceph-node1，便于区分。

  进入主机后，输入以下命令查看主机名：

  ```
  chen@chen-virtual-machine:~$ hostnamectl
     Static hostname: chen-virtual-machine
           Icon name: computer-vm
             Chassis: vm
          Machine ID: 55bea4ada4f242edaf1a3d96c629acc0
             Boot ID: c8f1c332bc694b71b513cd999de0b550
      Virtualization: vmware
    Operating System: Ubuntu 18.04.6 LTS
              Kernel: Linux 5.4.0-89-generic
        Architecture: x86-64
  ```

  可以从以上看出，虚拟机的hostname为chen-virtual-machine，此时我们要更改虚拟机的hostname：

  ```
  sudo vi /etc/hostname
  ```

  在hostname文件中修改为我们所需要的ceph-node1，输入：wq！退出，再重启虚拟机后，可以看到主机的名字已经更改。

  ```
     Static hostname: ceph-node1
           Icon name: computer-vm
             Chassis: vm
          Machine ID: 55bea4ada4f242edaf1a3d96c629acc0
             Boot ID: b700ec3f168b4642bff033fb6d44ead8
      Virtualization: vmware
    Operating System: Ubuntu 18.04.6 LTS
              Kernel: Linux 5.4.0-89-generic
        Architecture: x86-64
  ```

  以上步骤完成主机的创建，再通过vmware虚拟机的克隆功能，再创建出ceph-node2，ceph-node3，ceph-client三台主机，由此，我们得到以下的四台主机。

  ```
  ip地址              主机名           ceph磁盘          备注   
  192.168.109.131      ceph-node1       20G               作为mds、mon、osd0
  192.168.109.132      ceph-node2       20G               作为osd1
  192.168.109.133      ceph-node3       20G               作为osd2
  192.168.109.134      ceph-client      挂载点：/cephfs   ceph客户端
  ```

  

* ### 添加host

  分别在ceph的三个节点机（ceph-node1、ceph-node2、ceph-node3）上添加hosts

  ```
  127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
  ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
  192.168.109.131 ceph-node1
  192.168.109.132 ceph-node2
  192.168.109.133 ceph-node3
  ```

  添加完hosts后，做下测试，保证使用hosts中映射的主机名能ping通。

  ```
  chen@ceph-node1:~$ ping ceph-node2
  PING ceph-node2 (192.168.109.132) 56(84) bytes of data.
  64 bytes from ceph-node2 (192.168.109.132): icmp_seq=1 ttl=64 time=0.821 ms
  64 bytes from ceph-node2 (192.168.109.132): icmp_seq=2 ttl=64 time=0.441 ms
  64 bytes from ceph-node2 (192.168.109.132): icmp_seq=3 ttl=64 time=0.308 ms
  64 bytes from ceph-node2 (192.168.109.132): icmp_seq=4 ttl=64 time=0.432 ms
  64 bytes from ceph-node2 (192.168.109.132): icmp_seq=5 ttl=64 time=0.448 ms
  64 bytes from ceph-node2 (192.168.109.132): icmp_seq=6 ttl=64 time=0.300 ms
  64 bytes from ceph-node2 (192.168.109.132): icmp_seq=7 ttl=64 time=0.513 ms
  ^C
  --- ceph-node2 ping statistics ---
  7 packets transmitted, 7 received, 0% packet loss, time 6132ms
  rtt min/avg/max/mdev = 0.300/0.466/0.821/0.162 ms
  ```

  ```
  chen@ceph-node1:~$ ping ceph-node3
  PING ceph-node3 (192.168.109.133) 56(84) bytes of data.
  64 bytes from ceph-node3 (192.168.109.133): icmp_seq=1 ttl=64 time=1.15 ms
  64 bytes from ceph-node3 (192.168.109.133): icmp_seq=2 ttl=64 time=0.422 ms
  64 bytes from ceph-node3 (192.168.109.133): icmp_seq=3 ttl=64 time=0.843 ms
  64 bytes from ceph-node3 (192.168.109.133): icmp_seq=4 ttl=64 time=0.339 ms
  64 bytes from ceph-node3 (192.168.109.133): icmp_seq=5 ttl=64 time=0.620 ms
  64 bytes from ceph-node3 (192.168.109.133): icmp_seq=6 ttl=64 time=0.519 ms
  ^C
  --- ceph-node3 ping statistics ---
  6 packets transmitted, 6 received, 0% packet loss, time 5051ms
  rtt min/avg/max/mdev = 0.339/0.649/1.155/0.278 ms
  ```

* ### 创建用户ceph

  分别在ceph的三个节点机（ceph-node1、ceph-node2、ceph-node3）上创建用户ceph，密码统一设置为ceph

  ```
  root@ceph-node1:/home/chen# adduser ceph
  Adding user `ceph' ...
  Adding new group `ceph' (1003) ...
  Adding new user `ceph' (1003) with group `ceph' ...
  Creating home directory `/home/ceph' ...
  Copying files from `/etc/skel' ...
  Enter new UNIX password: 
  Retype new UNIX password: 
  passwd: password updated successfully
  Changing the user information for ceph
  Enter the new value, or press ENTER for the default
  	Full Name []: 
  	Room Number []: 
  	Work Phone []: 
  	Home Phone []: 
  	Other []: 
  Is the information correct? [Y/n] Y
  ```

  在每个Ceph节点中为用户增加 root 权限

  ```
  root@ceph-node1:/home/chen# echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
  ceph ALL = (root) NOPASSWD:ALL
  root@ceph-node1:/home/chen# chmod 0440 /etc/sudoers.d/ceph
  ```

  测试是否具有sudo权限

  ```
  root@ceph-node1:/home/chen# su - ceph
  ceph@ceph-node1:~$ sudo su -
  root@ceph-node1:~# 
  ```

  关闭防火墙

  ```
  [root@ceph-node1 ~]# service iptables stop
  [root@ceph-node1 ~]# chkconfig iptables off
  [root@ceph-node1 ~]# setenforce 0
  [root@ceph-node1 ~]# getenforce
  Permissive
  [root@ceph-node1 ~]# sed -i 's_SELINUX=enforcing_SELINUX=disabled_g' /etc/sysconfig/selinux
  ```

* ### 管理节点部署设置

  这个只需要在ceph-node1监控节点操作即可。增加Ceph资料库至 ceph-deploy 管理节点，之后安装 ceph-deploy。

  增加Ceph资料库至 ceph-deploy 管理节点，之后安装 ceph-deploy。
  
  ```
  [root@ceph-node1 ~]# rpm -Uvh http://download.ceph.com/rpm-hammer/el6/noarch/ceph-release-1-1.el6.noarch.rpm
  [root@ceph-node1 ~]# yum install ceph-deploy -y
  [root@ceph-node1 ~]# pip install ceph-deploy 
  ```

  创建集群：
  
  ```
  ceph-deploy new {initial-monitor-node(s)}
  ```
  
  当前目录下用 `ls` 和 `cat` 检查 `ceph-deploy` 的输出，应该有一个 Ceph 配置文件、一个 monitor 密钥环和一个日志文件
  
  把 Ceph 配置文件里的默认副本数从 `3` 改成 `2` ，这样只有两个 OSD 也可以达到 `active + clean` 状态。把下面这行加入 `[global]` 段：
  
  ```
  osd pool default size = xxxxxxxxxx osd pool default size = 2java -version
  ```
  
  安装 Ceph,`ceph-deploy` 将在各节点安装 Ceph
  
  ```
  ceph-deploy install ceph-node1 ceph-node2 ceph-node3
  ```
  
  完成上述操作后，当前目录里应该会出现这些密钥环：
  
  ```
  {cluster-name}.client.admin.keyring
  {cluster-name}.bootstrap-osd.keyring
  {cluster-name}.bootstrap-mds.keyring
  {cluster-name}.bootstrap-rgw.keyring
  ```
  
  添加两个 OSD 。
  
  ```
  ssh node2
  sudo mkdir /var/local/osd0
  exit
  
  ssh node3
  sudo mkdir /var/local/osd1
  exit
  ```
  
  然后，从管理节点执行 `ceph-deploy` 来准备 OSD
  
  ```
  ceph-deploy osd create ceph-node2:/path/to/directory
  ```
  
  最后，激活 OSD 。
  
  ```
  ceph-deploy osd activate {ceph-node}:/path/to/directory
  ```
  
  用 `ceph-deploy` 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 `ceph.client.admin.keyring` 了
  
  ```
  ceph-deploy admin  ceph-node1 ceph-node2 ceph-node3
  ```
  
  确保你对 `ceph.client.admin.keyring` 有正确的操作权限
  
  ```
  sudo chmod +r /etc/ceph/ceph.client.admin.keyring
  ```
  
  检查集群的健康状况。
  
  ```
  ceph health
  ```
  
  ```
  [ceph@ceph-node1 ~]$ ceph health
  HEALTH_OK
  [ceph@ceph-node1 ~]$ ceph -s
      cluster 12d4f209-69ea-4ad9-9507-b00557b42815
       health HEALTH_OK
       monmap e1: 1 mons at {ceph-node1=192.168.10.200:6789/0}
              election epoch 2, quorum 0 ceph-node1
       osdmap e14: 3 osds: 3 up, 3 in
        pgmap v26: 64 pgs, 1 pools, 0 bytes data, 0 objects
              15459 MB used, 45944 MB / 61404 MB avail
                    64 active+clean
  ```
  

## 过程中遇到的问题

- #### 使用rpm安装出现error: Failed dependencies的解决方法

  在安装包后面加两个参数

  ```
  --nodeps --force
  ```

  其作用为不再分析包之间的依赖关系而直接安装

- #### ceph- deply 安装出现如下问题
  RuntimeError: Failed to execute command: env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ceph ceph-osd ceph-mds ceph-mon radosgw

  发现是ceph-deply的安装有问题，于是尝试卸载ceph-deply重新安装，但是卸载时报错，得先修复ceph-base的依赖，再使用命令apt --fix-broken install报错

  ```
  E: Sub-process /usr/bin/dpkg returned an error code (1)
  ```

  于是通过以下命令进行修复

  ```
  cd /var/lib/dpkg/
  2 sudo mv info/ info_bak          # 现将info文件夹更名
  3 sudo mkdir info                 # 再新建一个新的info文件夹
  4 sudo apt-get update             # 更新
  5 sudo apt-get -f install         # 修复
  6 sudo mv info/* info_bak/        # 执行完上一步操作后会在新的info文件夹下生成一些文件，现将这些文件全部移到info_bak文件夹下
  7 sudo rm -rf info                # 把自己新建的info文件夹删掉
  8 sudo mv info_bak info           # 把以前的info文件夹重新改回名
  ```

- #### ceph-deploy mon create-initial报错admin_socket: exception getting command descriptions: [Errno 2] No such file or directory

  vim ceph.conf 最下面加入行：

  ```
  public_network= 192.168.109.0/24
  ```

  再执行以下命令：

  ```
  [root@ceph-node1]# ceph-deploy --overwrite-conf config push ceph-node1 ceph-node2 ceph-node3
  ```

  



